{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time  # to time the learning process\n",
    "import json  # to get the configuration of the environment\n",
    "from env.simple_road_env import Road\n",
    "from models.simple_brains import QLearningTable\n",
    "from models.simple_brains import DP\n",
    "from models.simple_DQN_tensorflow import DeepQNetwork\n",
    "from collections import deque\n",
    "import math\n",
    "from utils.logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reinforcement Learning example for the motion of an driving agent on a straight road.\n",
    "-   The brain (called \"RL\") is chosen among the different implementations in RL_brain.py\n",
    "-   The Environment in simple_road_env.py\n",
    "\n",
    "Discrete State Space\n",
    "-   see simple_road_env.py\n",
    "\n",
    "Action Space:\n",
    "-\t“Maintain” current lane and speed,\n",
    "-\t“Accelerate” at rate = a1[m/s2], provided velocity does not exceed vmax[km/h],\n",
    "-\t“Decelerate” at rate = −a1[m/s2], provided velocity is above vmin[km/h],\n",
    "-\t“Hard Accelerate” at rate = a2[m/s2], provided velocity does not exceed vmax[km/h],\n",
    "-\t“Hard Decelerate” at rate = −a2[m/s2], provided velocity is above vmin[km/h],\n",
    "(acceleration are given for a constant amount this time step)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# seed = np.random.seed(0)\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(using_tkinter, agent, method, gamma, learning_rate, eps_start, eps_end, eps_decay,\n",
    "                window_success, threshold_success, returns_list, steps_counter_list, info_training,\n",
    "                max_nb_episodes, max_nb_steps, sleep_time, folder_name=\"\"):\n",
    "    \"\"\"\n",
    "\n",
    "    :param using_tkinter: [bool] to display the environment, or not\n",
    "    :param agent: [brain object]\n",
    "    :param method: [string] value-based learning method - either sarsa or q-learning\n",
    "    :param gamma: [float between 0 and 1] discount factor\n",
    "    If gamma is closer to one, the agent will consider future rewards with greater weight,\n",
    "    willing to delay the reward.\n",
    "    :param learning_rate: [float between 0 and 1] - Non-constant learning rate must be used?\n",
    "    :param eps_start: [float]\n",
    "    :param eps_end: [float]\n",
    "    :param eps_decay: [float]\n",
    "    :param window_success: [int]\n",
    "    :param threshold_success: [float] to solve the env, = average score over the last x scores, where x = window_success\n",
    "    :param returns_list: [list of float]\n",
    "    :param steps_counter_list: [list of int]\n",
    "    :param info_training: [dict]\n",
    "    :param max_nb_episodes: [int] limit of training episodes\n",
    "    :param max_nb_steps: [int] maximum number of timesteps per episode\n",
    "    :param sleep_time: [int] sleep_time between two steps [ms]\n",
    "    :param folder_name: [string] to distinguish between runs during hyper-parameter tuning\n",
    "    :return: [list] returns_list - to be displayed\n",
    "    \"\"\"\n",
    "    returns_window = deque(maxlen=window_success)  # last x scores, where x = window_success\n",
    "\n",
    "    # probability of random choice for epsilon-greedy action selection\n",
    "    greedy_epsilon = eps_start\n",
    "\n",
    "    # record for each episode:\n",
    "    # steps_counter_list = []  # number of steps in each episode - look if some get to max_nb_steps\n",
    "    # returns_list = []  # return in each episode\n",
    "    best_trajectories_list = []\n",
    "\n",
    "    # track maximum return\n",
    "    max_return = -math.inf  # to be set low enough (setting max_nb_steps * max_cost_per_step should do it)\n",
    "    max_window = -np.inf\n",
    "\n",
    "    # initialize updated variable\n",
    "    current_action = None\n",
    "    next_observation = None\n",
    "\n",
    "    # measure the running time\n",
    "    time_start = time.time()\n",
    "    nb_episodes_seen = max_nb_episodes\n",
    "    #\n",
    "    for episode_id in range(max_nb_episodes):  # limit the number of episodes during training\n",
    "        # while episode_id < max_nb_episodes\n",
    "        # episode_id = episode_id + 1\n",
    "\n",
    "        # reset metrics\n",
    "        step_counter = max_nb_steps  # length of episode\n",
    "        return_of_episode = 0  # = score\n",
    "        trajectory = []  # sort of replay-memory, just for debugging\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        changes_in_state = 0\n",
    "        reward = 0\n",
    "        next_action = None\n",
    "\n",
    "        # reset the environment for a new episode\n",
    "        current_observation, masked_actions_list = env.reset()  # initial observation = initial state\n",
    "\n",
    "        # for sarsa - agent selects next action based on observation\n",
    "        if (method == \"sarsa\") or (method == \"sarsa_lambda\"):\n",
    "            current_action = agent.choose_action(current_observation, masked_actions_list, greedy_epsilon)\n",
    "            if method == \"sarsa_lambda\":\n",
    "                # for sarsa_lambda - initial all zero eligibility trace\n",
    "                agent.reset_eligibility_trace()\n",
    "\n",
    "        if method_used == \"mc_control\":\n",
    "            # generate an episode by following epsilon-greedy policy\n",
    "            episode = []\n",
    "            current_observation, _ = env.reset()\n",
    "            for step_id in range(max_nb_steps):  # while True\n",
    "                current_action = agent.choose_action(tuple(current_observation), masked_actions_list, greedy_epsilon)\n",
    "                next_observation, reward, termination_flag, masked_actions_list = env.step(current_action)\n",
    "                return_of_episode += reward\n",
    "\n",
    "                # a tuple is hashable and can be used in defaultdict\n",
    "                episode.append((tuple(current_observation), current_action, reward))\n",
    "                current_observation = next_observation\n",
    "\n",
    "                if termination_flag:\n",
    "                    step_counter = step_id\n",
    "                    steps_counter_list.append(step_id)\n",
    "                    returns_list.append(return_of_episode)\n",
    "                    break\n",
    "\n",
    "            # update the action-value function estimate using the episode\n",
    "            # print(\"episode = {}\".format(episode))\n",
    "            # agent.compare_reference_value()\n",
    "            agent.learn(episode, gamma, learning_rate)\n",
    "\n",
    "        else:  # TD\n",
    "            # run episodes\n",
    "            for step_id in range(max_nb_steps):\n",
    "                # ToDo: how to penalize the agent that does not terminate the episode?\n",
    "\n",
    "                # fresh env\n",
    "                if using_tkinter:\n",
    "                    env.render(sleep_time)\n",
    "\n",
    "                if (method == \"sarsa\") or (method == \"sarsa_lambda\"):\n",
    "                    next_observation, reward, termination_flag, masked_actions_list = env.step(current_action)\n",
    "                    return_of_episode += reward\n",
    "                    if not termination_flag:  # if done\n",
    "                        # Online-Policy: Choose an action At+1 following the same e-greedy policy based on current Q\n",
    "                        # ToDo: here, we should read the masked_actions_list associated to the next_observation\n",
    "                        masked_actions_list = env.masking_function(next_observation)\n",
    "                        next_action = agent.choose_action(next_observation, masked_actions_list=masked_actions_list,\n",
    "                                                          greedy_epsilon=greedy_epsilon)\n",
    "\n",
    "                        # agent learn from this transition\n",
    "                        agent.learn(current_observation, current_action, reward, next_observation, next_action,\n",
    "                                    termination_flag, gamma, learning_rate)\n",
    "                        current_observation = next_observation\n",
    "                        current_action = next_action\n",
    "\n",
    "                    if termination_flag:  # if done\n",
    "                        agent.learn(current_observation, current_action, reward, next_observation, next_action,\n",
    "                                    termination_flag, gamma, learning_rate)\n",
    "                        # ToDo: check it ignore next_observation and next_action\n",
    "                        step_counter = step_id\n",
    "                        steps_counter_list.append(step_id)\n",
    "                        returns_list.append(return_of_episode)\n",
    "                        break\n",
    "\n",
    "                elif (method == \"q\") or (method == \"expected_sarsa\") or (method == \"simple_dqn_pytorch\"):\n",
    "                    current_action = agent.choose_action(current_observation, masked_actions_list, greedy_epsilon)\n",
    "                    next_observation, reward, termination_flag, masked_actions_list = env.step(current_action)\n",
    "                    return_of_episode += reward\n",
    "\n",
    "                    if method == \"q\":\n",
    "                        # agent learn from this transition\n",
    "                        agent.learn(current_observation, current_action, reward, next_observation, termination_flag,\n",
    "                                    gamma, learning_rate)\n",
    "\n",
    "                    elif method == \"simple_dqn_pytorch\":\n",
    "                        agent.step(current_observation, current_action, reward, next_observation, termination_flag)\n",
    "\n",
    "                    elif method == \"expected_sarsa\":\n",
    "                        agent.learn(current_observation, current_action, reward, next_observation, termination_flag,\n",
    "                                    greedy_epsilon, gamma, learning_rate)\n",
    "\n",
    "                    else:  # DQN with tensorflow\n",
    "                        # New: store transition in memory - subsequently to be sampled from\n",
    "                        agent.store_transition(current_observation, current_action, reward, next_observation)\n",
    "\n",
    "                        # if the number of steps is larger than a threshold, start learn ()\n",
    "                        if (step_id > 5) and (step_id % 5 == 0):  # for 1 to T\n",
    "                            # print('learning')\n",
    "                            # pick up some transitions from the memory and learn from these samples\n",
    "                            agent.learn()\n",
    "\n",
    "                    current_observation = next_observation\n",
    "\n",
    "                    if termination_flag:  # if done\n",
    "                        step_counter = step_id\n",
    "                        steps_counter_list.append(step_id)\n",
    "                        returns_list.append(return_of_episode)\n",
    "                        # agent.compare_reference_value()\n",
    "\n",
    "                        break\n",
    "\n",
    "                # log\n",
    "                trajectory.append(current_observation)\n",
    "                trajectory.append(current_action)\n",
    "\n",
    "                # monitor actions, states and rewards are not constant\n",
    "                rewards.append(reward)\n",
    "                actions.append(current_action)\n",
    "                if not (next_observation[0] == current_observation[0]\n",
    "                        and next_observation[1] == current_observation[1]):\n",
    "                    changes_in_state = changes_in_state + 1\n",
    "\n",
    "        # At this point, the episode is terminated\n",
    "        # decay epsilon\n",
    "        greedy_epsilon = max(eps_end, eps_decay * greedy_epsilon)\n",
    "\n",
    "        # log\n",
    "        trajectory.append(next_observation)  # final state\n",
    "        returns_window.append(return_of_episode)  # save most recent score\n",
    "        if episode_id % 100 == 0:\n",
    "            time_intermediate = time.time()\n",
    "            print('\\n --- Episode={} ---\\n eps={}\\n Average Score in returns_window = {:.2f} \\n duration={:.2f}'.format(\n",
    "                episode_id, greedy_epsilon, np.mean(returns_window), time_intermediate - time_start))\n",
    "            # agent.print_q_table()\n",
    "\n",
    "        if episode_id % 20 == 0:\n",
    "            print('Episode {} / {}. Eps = {}. Total_steps = {}. Return = {}. Max return = {}, Top 10 = {}'.format(\n",
    "                episode_id+1, max_nb_episodes, greedy_epsilon, step_counter, return_of_episode, max_return,\n",
    "                sorted(returns_list, reverse=True)[:10]))\n",
    "\n",
    "        if return_of_episode == max_return:\n",
    "            if trajectory not in best_trajectories_list:\n",
    "                best_trajectories_list.append(trajectory)\n",
    "        elif return_of_episode > max_return:\n",
    "            del best_trajectories_list[:]\n",
    "            best_trajectories_list.append(trajectory)\n",
    "            max_return = return_of_episode\n",
    "\n",
    "        if np.mean(returns_window) > max_window:\n",
    "            max_window = np.mean(returns_window)\n",
    "\n",
    "        # test success\n",
    "        if np.mean(returns_window) >= threshold_success:\n",
    "            time_stop = time.time()\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}, duration={:.2f} [s]'.format(\n",
    "                episode_id - window_success, np.mean(returns_window), time_stop - time_start))\n",
    "            info_training[\"nb_episodes_to_solve\"] = episode_id - window_success\n",
    "            nb_episodes_seen = episode_id\n",
    "            break\n",
    "\n",
    "    time_stop = time.time()\n",
    "    info_training[\"duration\"] = int(time_stop - time_start)\n",
    "    info_training[\"nb_episodes_seen\"] = nb_episodes_seen\n",
    "    info_training[\"final_epsilon\"] = greedy_epsilon\n",
    "    info_training[\"max_window\"] = max_window\n",
    "    info_training[\"reference_values\"] = agent.compare_reference_value()\n",
    "\n",
    "    # where to save the weights\n",
    "    parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "    folder = os.path.join(parent_dir, \"results/simple_road/\" + folder_name)\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    agent.save_q_table(folder)\n",
    "\n",
    "    print('End of training')\n",
    "    print('Best return : %s --- with %s different trajectory(ies)' % (max_return, len(best_trajectories_list)))\n",
    "    for trajectory in best_trajectories_list:\n",
    "        print(trajectory)\n",
    "\n",
    "    if using_tkinter:\n",
    "        env.destroy()\n",
    "\n",
    "    # return returns_list, steps_counter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(agent, method_used_to_plot, returns_to_plot, smoothing_window, threshold_success,\n",
    "                    steps_counter_list_to_plot, display_flag=True, folder_name=\"\"):\n",
    "    \"\"\"\n",
    "    Use to SAVE + plot (optional)\n",
    "    :param agent:\n",
    "    :param method_used_to_plot:\n",
    "    :param returns_to_plot:\n",
    "    :param smoothing_window:\n",
    "    :param threshold_success:\n",
    "    :param steps_counter_list_to_plot:\n",
    "    :param display_flag:\n",
    "    :param folder_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # where to save the plots\n",
    "    parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "    folder = os.path.join(parent_dir, \"results/simple_road/\" + folder_name)\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    # plot step_counter for each episode\n",
    "    plt.figure()\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.title(\"Episode Step_counts over Time (Smoothed over window size {})\".format(smoothing_window))\n",
    "    plt.ylabel(\"Episode step_count (Smoothed)\")\n",
    "    steps_smoothed = pd.Series(steps_counter_list_to_plot).rolling(\n",
    "        smoothing_window, min_periods=smoothing_window).mean()\n",
    "    plt.plot(steps_counter_list_to_plot, linewidth=0.5)\n",
    "    plt.plot(steps_smoothed, linewidth=2.0)\n",
    "    plt.savefig(folder + \"step_counter.png\", dpi=800)\n",
    "    if display_flag:\n",
    "        plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.grid(True)\n",
    "    returns_smoothed = pd.Series(returns_to_plot).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "    plt.plot(returns_to_plot, linewidth=0.5)\n",
    "    plt.plot(returns_smoothed, linewidth=2.0)\n",
    "    plt.axhline(y=threshold_success, color='r', linestyle='-')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Return(Smoothed)\")\n",
    "    plt.title(\"Episode Return over Time (Smoothed over window size {})\".format(smoothing_window))\n",
    "    plt.savefig(folder + \"return.png\", dpi=800)\n",
    "    if display_flag:\n",
    "        plt.show()\n",
    "\n",
    "    # bins = range(min(returns_to_plot), max(returns_to_plot) + 1, 1)\n",
    "    plt.figure()\n",
    "    plt.hist(returns_to_plot, norm_hist=True, bins=100)\n",
    "    plt.ylabel('reward distribution')\n",
    "    if display_flag:\n",
    "        plt.show()\n",
    "\n",
    "    agent.print_q_table()\n",
    "    if method_used_to_plot not in [\"simple_dqn_tensorflow\", \"simple_dqn_pytorch\", \"mc_control\"]:\n",
    "        agent.plot_q_table(folder, display_flag)\n",
    "        agent.plot_optimal_actions_at_each_position(folder, display_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(using_tkinter_test, agent, returns_list, nb_episodes=1, max_nb_steps=20, sleep_time=0.001,\n",
    "               weight_file_name=\"q_table.pkl\"):\n",
    "    \"\"\"\n",
    "    load weights and show one run\n",
    "    :param using_tkinter_test: [bool]\n",
    "    :param agent: [brain object]\n",
    "    :param returns_list: [float list] - argument passed by reference\n",
    "    :param nb_episodes: [int]\n",
    "    :param max_nb_steps: [int]\n",
    "    :param sleep_time: [float]\n",
    "    :param weight_file_name: [string]\n",
    "    :return: -\n",
    "    \"\"\"\n",
    "    grand_parent_dir_test = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "    weight_file = os.path.abspath(grand_parent_dir_test + \"/results/simple_road/\" + weight_file_name)\n",
    "    if agent.load_q_table(weight_file):\n",
    "\n",
    "        for episode_id in range(nb_episodes):\n",
    "            trajectory = []\n",
    "            # reset the environment for a new episode\n",
    "            current_observation, masked_actions_list = env.reset()  # initial observation = initial state\n",
    "            print(\"{} = initial_observation\".format(current_observation))\n",
    "            score = 0  # initialize the score\n",
    "            step_id = 0\n",
    "            while step_id < max_nb_steps:\n",
    "                step_id += 1\n",
    "                # fresh env\n",
    "                if using_tkinter_test:\n",
    "                    env.render(sleep_time)\n",
    "\n",
    "                # agent choose current_action based on observation\n",
    "                greedy_epsilon = 0\n",
    "                current_action = agent.choose_action(current_observation, masked_actions_list, greedy_epsilon)\n",
    "\n",
    "                next_observation, reward, termination_flag, masked_actions_list = env.step(current_action)\n",
    "\n",
    "                score += reward  # update the score\n",
    "\n",
    "                trajectory.append(current_observation)\n",
    "                trajectory.append(current_action)\n",
    "                trajectory.append(reward)\n",
    "                trajectory.append(termination_flag)\n",
    "\n",
    "                # update state\n",
    "                current_observation = next_observation\n",
    "                print(\"\\r {}, {}, {}.\".format(current_action, reward, termination_flag), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                if termination_flag:  # exit loop if episode finished\n",
    "                    trajectory.append(next_observation)\n",
    "                    break\n",
    "\n",
    "            returns_list.append(score)\n",
    "            print(\"\\n{}/{} - Return: {}\".format(episode_id, nb_episodes, score))\n",
    "            print(\"\\nTrajectory = {}\".format(trajectory))\n",
    "            # Best trajectory= [[0, 3], 'no_change', [3, 3], 'no_change', [6, 3], 'no_change', [9, 3], 'slow_down',\n",
    "            # [11, 2], 'no_change', [13, 2], 'speed_up', [16, 3], 'no_change', [19, 3]]\n",
    "\n",
    "        print(\"---\")\n",
    "        print(\"{} = average return\".format(np.mean(returns_list)))\n",
    "    else:\n",
    "        print(\"cannot load weight_file at {}\".format(weight_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'environments/simple_road_env_configuration.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-96816b584d4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mdict_configuration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# saving the configuration in a json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'environments/simple_road_env_configuration.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_configuration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'environments/simple_road_env_configuration.json'"
     ]
    }
   ],
   "source": [
    "actions_list = [\"no_change\", \"speed_up\", \"speed_up_up\", \"slow_down\", \"slow_down_down\"]\n",
    "state_features_list = [\"position\", \"velocity\"]  # , \"obstacle_position\"]\n",
    "\n",
    "# the environment\n",
    "flag_tkinter = False\n",
    "initial_state = [0, 3, 12]\n",
    "goal_velocity = 3\n",
    "env = Road(flag_tkinter, actions_list, state_features_list, initial_state, goal_velocity)\n",
    "\n",
    "# getting the configuration of the test\n",
    "env_configuration = vars(env)\n",
    "dict_configuration = dict(env_configuration)\n",
    "\n",
    "# avoid special types:\n",
    "not_to_consider = [\"tk\", \"children\", \"canvas\", \"_tclCommands\", \"master\", \"_tkloaded\", \"colour_action_code\",\n",
    "                   \"colour_velocity_code\", \"origin_coord\", \"display_canvas\", \"origin\", \"_last_child_ids\", \"rect\",\n",
    "                   \"logger\"]\n",
    "for elem in not_to_consider:\n",
    "    if elem in dict_configuration:\n",
    "        del dict_configuration[elem]\n",
    "# saving the configuration in a json\n",
    "with open('env/simple_road_env_configuration.json', 'w') as outfile:\n",
    "    json.dump(dict_configuration, outfile)\n",
    "\n",
    "# -2- deep TD to update the state-action table:\n",
    "method_used = \"simple_dqn_tensorflow\"\n",
    "\n",
    "# -3- Model-Based Dynamic Programming\n",
    "# Dynamic programming assumes that the agent has full knowledge of the MDP\n",
    "# method_used = \"DP\"\n",
    "\n",
    "grand_parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "results_dir = os.path.abspath(grand_parent_dir + \"/results/simple_road/\")\n",
    "print(\"results_dir = {}\".format(results_dir))\n",
    "brain_agent = DeepQNetwork(actions=actions_list,\n",
    "                           state=state_features_list,\n",
    "                           learning_rate=0.01,\n",
    "                           reward_decay=0.9,\n",
    "                           # e_greedy=0.9,\n",
    "                           replace_target_iter=300,  # replace net parameters every X learning\n",
    "                           memory_size=50,\n",
    "                           summaries_dir=results_dir,\n",
    "                           # saver_dir='/tmp/tensorflow_logs/RL/saver/'\n",
    "                           saver_dir=None\n",
    "                           )\n",
    "# elif method_used == \"DP\":\n",
    "#     # make sure it does not do any training or testing (it has all its methods are implemented internally)\n",
    "#     brain_agent = DP(actions=actions_list, state=state_features_list, env=env, gamma=0.9)\n",
    "\n",
    "#     # ToDo: Problem at state = [3, 3]\n",
    "#     # q_values_for_this_state = [8.40 9.23 -inf 4.76 -2.11] makes the agent go for speed_up => Best = 17 (not 18)\n",
    "\n",
    "#     # check the interface with the environment through specific values\n",
    "#     final_state = [19, 3]\n",
    "#     final_action = \"no_change\"\n",
    "#     next_observation_dp, reward_dp, termination_flag_dp = brain_agent.get_value_from_state(final_state,\n",
    "#                                                                                            final_action)\n",
    "#     action = \"no_change\"\n",
    "#     obstacle_state = [12, 2]\n",
    "#     next_observation_dp, reward_dp, termination_flag_dp = brain_agent.get_value_from_state(obstacle_state, action)\n",
    "#     print(\" {}, {}, {} = results\".format(next_observation_dp, reward_dp, termination_flag_dp))\n",
    "\n",
    "#     # compare value_iteration and policy_iteration\n",
    "#     opt_policy_pi, opt_v_table_pi = brain_agent.policy_iteration()\n",
    "#     np.save('opt_policy_pi.npy', opt_policy_pi)\n",
    "#     np.save('opt_v_table_pi.npy', opt_v_table_pi)\n",
    "#     opt_q_table_pi = brain_agent.q_from_v(opt_v_table_pi)\n",
    "#     np.save('opt_q_table_pi.npy', opt_q_table_pi)\n",
    "#     print(\"final_state_values p_i = {}\".format(opt_q_table_pi[final_state[0]][final_state[1]]))\n",
    "#     print(opt_v_table_pi)\n",
    "#     print(opt_q_table_pi)\n",
    "\n",
    "#     # opt_policy_pi = np.load('opt_policy_pi.npy')\n",
    "#     return_of_episode_pi, trajectory_pi = brain_agent.run_policy(opt_policy_pi, [0, 3])\n",
    "#     print(\"p_i has return = {} for trajectory = {}\".format(return_of_episode_pi, trajectory_pi))\n",
    "\n",
    "#     print(\"\\n --- \\n\")\n",
    "\n",
    "#     opt_policy_vi, opt_v_table_vi = brain_agent.value_iteration()\n",
    "#     np.save('opt_policy_vi.npy', opt_policy_vi)\n",
    "#     np.save('opt_v_table_vi.npy', opt_v_table_vi)\n",
    "#     opt_q_table_vi = brain_agent.q_from_v(opt_v_table_vi)\n",
    "#     np.save('opt_q_table_vi.npy', opt_q_table_vi)\n",
    "#     print(\"final_state_values v_i = {}\".format(opt_q_table_vi[final_state[0]][final_state[1]]))\n",
    "#     print(opt_v_table_vi)\n",
    "#     print(opt_q_table_vi)\n",
    "\n",
    "#     return_of_episode_vi, trajectory_vi = brain_agent.run_policy(opt_policy_vi, [0, 3])\n",
    "#     print(\"v_i has return = {} for trajectory = {}\".format(return_of_episode_vi, trajectory_vi))\n",
    "\n",
    "# Training and/or Testing\n",
    "flag_training_once = True\n",
    "flag_testing = False\n",
    "flag_training_hyper_parameter_tuning = False  # Tkinter is not used when tuning hyper-parameters\n",
    "display_learning_results = False  # only used for training_once\n",
    "\n",
    "# for testing\n",
    "max_nb_steps_testing = 50\n",
    "nb_tests = 10\n",
    "sleep_time_between_steps_testing = 0.5  # slow to see the steps\n",
    "\n",
    "# for learning\n",
    "# hyper-parameters\n",
    "gamma_learning = 0.99\n",
    "learning_rate_learning = 0.02\n",
    "eps_start_learning = 1.0\n",
    "eps_end_training = 0.01\n",
    "# reach eps_end at episode_id = log10(eps_end/eps_start) / log10(eps_decay)\n",
    "# 0.99907 for 5000 at 0.01/1.0\n",
    "eps_decay_training = 0.998466\n",
    "# eps_decay_training = 0.99907  # - when 70000 episode\n",
    "# 0.99907  # for getting to 0.01 in ~5000 episodes\n",
    "\n",
    "# to reach eps_end at episode episode_id, eps_decay = (eps_end / eps_start) ** (1/episode_id)\n",
    "max_nb_episodes_training = 7000\n",
    "max_nb_steps_training = 25\n",
    "sleep_time_between_steps_learning = 0.0005\n",
    "\n",
    "# success conditions\n",
    "window_success_res = 100\n",
    "threshold_success_training = 17\n",
    "dict_info_training = {}\n",
    "# 22.97 for self.reward = 1 + self.reward / max(self.rewards_dict.values())\n",
    "# q_max = 9.23562904132267 for expected_sarsa\n",
    "\n",
    "if flag_training_hyper_parameter_tuning:\n",
    "\n",
    "    # No tkinter used\n",
    "    learning_rate_list = [0.003, 0.01, 0.03, 0.1, 0.3, 1]\n",
    "\n",
    "    gamma_learning_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99, 1]\n",
    "    nb_episodes_to_plateau_list = [300, 500, 800, 1000, 3000, 5000]\n",
    "    # [0.954992586021, 0.9847666521101, 0.995405417351, 0.998466120868, 0.9995395890030, 0.9999846495505]\n",
    "    eps_decay_list = [(eps_end_training / eps_start_learning) ** (1/nb) for nb in nb_episodes_to_plateau_list]\n",
    "\n",
    "    for i, param in enumerate(eps_decay_list):\n",
    "        brain_agent.reset_q_table()  # re-initialize the model!!\n",
    "\n",
    "        folder_name_training = str(i) + '/'\n",
    "        logger_name = str(i) + '.log'\n",
    "        logger = Logger(folder_name_training, logger_name, 0)\n",
    "\n",
    "        hyper_parameters = (\n",
    "            method_used,\n",
    "            gamma_learning,\n",
    "            learning_rate_learning,\n",
    "            eps_start_learning,\n",
    "            eps_end_training,\n",
    "            param  # decay\n",
    "        )\n",
    "        logger.log(str(hyper_parameters), 1)\n",
    "        # after = Register an alarm callback that is called after a given time.\n",
    "        # give results as reference\n",
    "        returns_list_res, steps_counter_list_res = [], []\n",
    "        dict_info_training = {}\n",
    "\n",
    "        train_agent(flag_tkinter, brain_agent, *hyper_parameters,\n",
    "                    window_success_res, threshold_success_training, returns_list_res,\n",
    "                    steps_counter_list_res, dict_info_training,\n",
    "                    max_nb_episodes_training, max_nb_steps_training, sleep_time_between_steps_learning,\n",
    "                    folder_name_training)\n",
    "        logger.log(dict_info_training, 1)\n",
    "\n",
    "        try:\n",
    "            display_results(brain_agent, method_used, returns_list_res, window_success_res,\n",
    "                            threshold_success_training, steps_counter_list_res,\n",
    "                            display_flag=False, folder_name=folder_name_training)\n",
    "        except Exception as e:\n",
    "            print('Exception = {}'.format(e))\n",
    "\n",
    "        # testing\n",
    "        returns_list_testing = []  # passed as a reference\n",
    "        test_agent(flag_tkinter, brain_agent, returns_list_testing, nb_tests, max_nb_steps_testing,\n",
    "                   sleep_time_between_steps_learning, folder_name_training + \"q_table.pkl\")\n",
    "        logger.log(returns_list_testing, 1)\n",
    "\n",
    "if flag_training_once:\n",
    "    hyper_parameters = (\n",
    "        method_used,\n",
    "        gamma_learning,\n",
    "        learning_rate_learning,\n",
    "        eps_start_learning,\n",
    "        eps_end_training,\n",
    "        eps_decay_training\n",
    "    )\n",
    "    print(\"hyper_parameters = {}\".format(hyper_parameters))\n",
    "    returns_list_res, steps_counter_list_res = [], []\n",
    "    if flag_tkinter:\n",
    "        # after(self, time [ms] before execution of func(*args), func=None, *args):\n",
    "        # !! callback function. No return value can be read\n",
    "        env.after(100, train_agent, flag_tkinter, brain_agent,\n",
    "                  *hyper_parameters,\n",
    "                  window_success_res, threshold_success_training, returns_list_res,\n",
    "                  steps_counter_list_res, dict_info_training,\n",
    "                  max_nb_episodes_training, max_nb_steps_training, sleep_time_between_steps_learning)\n",
    "        env.mainloop()\n",
    "        print(f\"returns_list_res = {returns_list_res}, window_success_res = {\n",
    "            window_success_res}, steps_counter_list_res = {steps_counter_list_res}\")\n",
    "    else:\n",
    "        train_agent(flag_tkinter, brain_agent, *hyper_parameters,\n",
    "                    window_success_res, threshold_success_training, returns_list_res,\n",
    "                    steps_counter_list_res, dict_info_training,\n",
    "                    max_nb_episodes_training, max_nb_steps_training, sleep_time_between_steps_learning)\n",
    "    try:\n",
    "        display_results(brain_agent, method_used, returns_list_res, window_success_res,\n",
    "                        threshold_success_training, steps_counter_list_res,\n",
    "                        display_flag=display_learning_results)\n",
    "    except Exception as e:\n",
    "        print('Exception = {}'.format(e))\n",
    "    print(\"hyper_parameters = {}\".format(hyper_parameters))\n",
    "\n",
    "    # print(brain_agent.reference_list)\n",
    "\n",
    "if flag_testing:\n",
    "    returns_list_testing = []\n",
    "    if flag_tkinter:\n",
    "        env.after(100, test_agent, flag_tkinter, brain_agent, returns_list_testing, nb_tests, max_nb_steps_testing,\n",
    "                  sleep_time_between_steps_testing)\n",
    "        env.mainloop()\n",
    "    else:\n",
    "        test_agent(flag_tkinter, brain_agent, returns_list_testing, nb_tests, max_nb_steps_testing,\n",
    "                   sleep_time_between_steps_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
