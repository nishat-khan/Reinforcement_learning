{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning example for the motion of an driving agent on a straight road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete State Space\n",
    "-   see simple_road_env.py\n",
    "\n",
    "Action Space:\n",
    "-\t“Maintain” current lane and speed,\n",
    "-\t“Accelerate” at rate = a1[m/s2], provided velocity does not exceed vmax[km/h],\n",
    "-\t“Decelerate” at rate = −a1[m/s2], provided velocity is above vmin[km/h],\n",
    "-\t“Hard Accelerate” at rate = a2[m/s2], provided velocity does not exceed vmax[km/h],\n",
    "-\t“Hard Decelerate” at rate = −a2[m/s2], provided velocity is above vmin[km/h],\n",
    "(acceleration are given for a constant amount this time step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time  # to time the learning process\n",
    "import json  # to get the configuration of the environment\n",
    "from env.simple_road_env import Road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.training_agent import train_agent\n",
    "from models.dp_q_models import QLearningTable\n",
    "from utils.visualization import display_results\n",
    "from collections import deque\n",
    "import math\n",
    "from utils.logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = np.random.seed(0)\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "np.set_printoptions(formatter={'float': lambda x: f\"{x:0.2f}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset_q_table - self.q_table has shape = (0, 7)\n",
      "hyper_parameters = ('q', 0.99, 0.02, 1.0, 0.01, 0.998466)\n",
      "\n",
      " --- Episode=0 ---\n",
      " eps=0.998466\n",
      " Average Score in returns_window = -142.00 \n",
      " duration=0.06\n",
      "Episode 1 / 100. Eps = 0.998466. Total_steps = 12. Return = -142. Max return = -inf, Top 10 = [-142]\n",
      "Episode 21 / 100. Eps = 0.9682753947729185. Total_steps = 8. Return = -86. Max return = 10, Top 10 = [10, 9, -18, -79, -86, -94, -98, -104, -109, -121]\n",
      "Episode 41 / 100. Eps = 0.9389976625369826. Total_steps = 5. Return = -149. Max return = 10, Top 10 = [10, 9, 3, -15, -18, -36, -42, -57, -79, -86]\n",
      "Episode 61 / 100. Eps = 0.9106052007618131. Total_steps = 12. Return = -178. Max return = 10, Top 10 = [10, 9, 7, 7, 3, -15, -18, -36, -38, -42]\n",
      "Episode 81 / 100. Eps = 0.8830712415344311. Total_steps = 8. Return = -126. Max return = 10, Top 10 = [10, 9, 7, 7, 3, -7, -8, -10, -15, -17]\n",
      "Saved as q_table.pkl\n",
      "End of training\n",
      "Best return : 10 --- with 1 different trajectory(ies)\n",
      "[[3, 3], 'no_change', [5, 2], 'slow_down', [9, 4], 'speed_up_up', [11, 2], 'slow_down_down', [13, 2], 'no_change', [16, 3], 'speed_up', [19, 3]]\n",
      "Exception = name 'parent_dir' is not defined\n",
      "hyper_parameters = ('q', 0.99, 0.02, 1.0, 0.01, 0.998466)\n"
     ]
    }
   ],
   "source": [
    "actions_list = [\"no_change\", \"speed_up\", \"speed_up_up\", \"slow_down\", \"slow_down_down\"]\n",
    "state_features_list = [\"position\", \"velocity\"]  # , \"obstacle_position\"]\n",
    "\n",
    "# the environment\n",
    "flag_tkinter = False\n",
    "initial_state = [0, 3, 12]\n",
    "goal_velocity = 3\n",
    "env = Road(flag_tkinter, actions_list, state_features_list, initial_state, goal_velocity)\n",
    "\n",
    "# getting the configuration of the test\n",
    "env_configuration = vars(env)\n",
    "dict_configuration = dict(env_configuration)\n",
    "\n",
    "# avoid special types:\n",
    "not_to_consider = [\"tk\", \"children\", \"canvas\", \"_tclCommands\", \"master\", \"_tkloaded\", \"colour_action_code\",\n",
    "                   \"colour_velocity_code\", \"origin_coord\", \"display_canvas\", \"origin\", \"_last_child_ids\", \"rect\",\n",
    "                   \"logger\"]\n",
    "for elem in not_to_consider:\n",
    "    if elem in dict_configuration:\n",
    "        del dict_configuration[elem]\n",
    "# saving the configuration in a json\n",
    "with open('env/simple_road_env_configuration.json', 'w') as outfile:\n",
    "    json.dump(dict_configuration, outfile)\n",
    "\n",
    "# Different possible algorithms to update the state-action table:\n",
    "\n",
    "# -1- Temporal-Difference  # all are working - \"q\" performs the best\n",
    "method_used = \"q\"\n",
    "\n",
    "# Instanciate the Agent\n",
    "brain_agent = QLearningTable(actions=actions_list, state=state_features_list, load_q_table=False)\n",
    "\n",
    "# Training and/or Testing\n",
    "flag_training_once = True\n",
    "flag_testing = False\n",
    "flag_training_hyper_parameter_tuning = False  # Tkinter is not used when tuning hyper-parameters\n",
    "display_learning_results = False  # only used for training_once\n",
    "\n",
    "# for testing\n",
    "max_nb_steps_testing = 50\n",
    "nb_tests = 10\n",
    "sleep_time_between_steps_testing = 0.5  # slow to see the steps\n",
    "\n",
    "# for learning\n",
    "# hyper-parameters\n",
    "gamma_learning = 0.99\n",
    "learning_rate_learning = 0.02\n",
    "eps_start_learning = 1.0\n",
    "eps_end_training = 0.01\n",
    "# reach eps_end at episode_id = log10(eps_end/eps_start) / log10(eps_decay)\n",
    "# 0.99907 for 5000 at 0.01/1.0\n",
    "eps_decay_training = 0.998466\n",
    "# eps_decay_training = 0.99907  # - when 70000 episode\n",
    "# 0.99907  # for getting to 0.01 in ~5000 episodes\n",
    "\n",
    "# to reach eps_end at episode episode_id, eps_decay = (eps_end / eps_start) ** (1/episode_id)\n",
    "max_nb_episodes_training = 100\n",
    "max_nb_steps_training = 25\n",
    "sleep_time_between_steps_learning = 0.0005\n",
    "\n",
    "# success conditions\n",
    "window_success_res = 100\n",
    "threshold_success_training = 17\n",
    "dict_info_training = {}\n",
    "# 22.97 for self.reward = 1 + self.reward / max(self.rewards_dict.values())\n",
    "# q_max = 9.23562904132267 for expected_sarsa\n",
    "\n",
    "if flag_training_hyper_parameter_tuning:\n",
    "\n",
    "    # No tkinter used\n",
    "    learning_rate_list = [0.003, 0.01, 0.03, 0.1, 0.3, 1]\n",
    "\n",
    "    gamma_learning_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99, 1]\n",
    "    nb_episodes_to_plateau_list = [300, 500, 800, 1000, 3000, 5000]\n",
    "    # [0.954992586021, 0.9847666521101, 0.995405417351, 0.998466120868, 0.9995395890030, 0.9999846495505]\n",
    "    eps_decay_list = [(eps_end_training / eps_start_learning) ** (1/nb) for nb in nb_episodes_to_plateau_list]\n",
    "\n",
    "    for i, param in enumerate(eps_decay_list):\n",
    "        brain_agent.reset_q_table()  # re-initialize the model!!\n",
    "\n",
    "        folder_name_training = str(i) + '/'\n",
    "        logger_name = str(i) + '.log'\n",
    "        logger = Logger(folder_name_training, logger_name, 0)\n",
    "\n",
    "        hyper_parameters = (\n",
    "            method_used,\n",
    "            gamma_learning,\n",
    "            learning_rate_learning,\n",
    "            eps_start_learning,\n",
    "            eps_end_training,\n",
    "            param  # decay\n",
    "        )\n",
    "        logger.log(str(hyper_parameters), 1)\n",
    "        # after = Register an alarm callback that is called after a given time.\n",
    "        # give results as reference\n",
    "        returns_list_res, steps_counter_list_res = [], []\n",
    "        dict_info_training = {}\n",
    "\n",
    "        train_agent(flag_tkinter, brain_agent, *hyper_parameters,\n",
    "                    window_success_res, threshold_success_training, returns_list_res,\n",
    "                    steps_counter_list_res, dict_info_training,\n",
    "                    max_nb_episodes_training, max_nb_steps_training, sleep_time_between_steps_learning,\n",
    "                    folder_name_training)\n",
    "        logger.log(dict_info_training, 1)\n",
    "\n",
    "        try:\n",
    "            display_results(brain_agent, method_used, returns_list_res, window_success_res,\n",
    "                            threshold_success_training, steps_counter_list_res,\n",
    "                            display_flag=False, folder_name=folder_name_training)\n",
    "        except Exception as e:\n",
    "            print(f'Exception = {e}')\n",
    "\n",
    "        # testing\n",
    "        returns_list_testing = []  # passed as a reference\n",
    "        test_agent(flag_tkinter, brain_agent, returns_list_testing, nb_tests, max_nb_steps_testing,\n",
    "                   sleep_time_between_steps_learning, folder_name_training + \"q_table.pkl\")\n",
    "        logger.log(returns_list_testing, 1)\n",
    "\n",
    "if flag_training_once:\n",
    "    hyper_parameters = (\n",
    "        method_used,\n",
    "        gamma_learning,\n",
    "        learning_rate_learning,\n",
    "        eps_start_learning,\n",
    "        eps_end_training,\n",
    "        eps_decay_training\n",
    "    )\n",
    "    print(\"hyper_parameters = {}\".format(hyper_parameters))\n",
    "    returns_list_res, steps_counter_list_res = [], []\n",
    "    if flag_tkinter:\n",
    "        # after(self, time [ms] before execution of func(*args), func=None, *args):\n",
    "        # !! callback function. No return value can be read\n",
    "        env.after(100, train_agent, flag_tkinter, brain_agent,\n",
    "                  *hyper_parameters,\n",
    "                  window_success_res, threshold_success_training, returns_list_res,\n",
    "                  steps_counter_list_res, dict_info_training,\n",
    "                  max_nb_episodes_training, max_nb_steps_training, sleep_time_between_steps_learning)\n",
    "        env.mainloop()\n",
    "        print(\"returns_list_res = {}, window_success_res = {}, steps_counter_list_res = {}\".format(\n",
    "            returns_list_res, window_success_res, steps_counter_list_res))\n",
    "    else:\n",
    "        train_agent(env, flag_tkinter, brain_agent, *hyper_parameters,\n",
    "                    window_success_res, threshold_success_training, returns_list_res,\n",
    "                    steps_counter_list_res, dict_info_training,\n",
    "                    max_nb_episodes_training, max_nb_steps_training, sleep_time_between_steps_learning)\n",
    "    try:\n",
    "        display_results(brain_agent, method_used, returns_list_res, window_success_res,\n",
    "                        threshold_success_training, steps_counter_list_res,\n",
    "                        display_flag=display_learning_results)\n",
    "    except Exception as e:\n",
    "        print('Exception = {}'.format(e))\n",
    "    print(\"hyper_parameters = {}\".format(hyper_parameters))\n",
    "\n",
    "    # print(brain_agent.reference_list)\n",
    "\n",
    "if flag_testing:\n",
    "    returns_list_testing = []\n",
    "    if flag_tkinter:\n",
    "        env.after(100, test_agent, flag_tkinter, brain_agent, returns_list_testing, nb_tests, max_nb_steps_testing,\n",
    "                  sleep_time_between_steps_testing)\n",
    "        env.mainloop()\n",
    "    else:\n",
    "        test_agent(flag_tkinter, brain_agent, returns_list_testing, nb_tests, max_nb_steps_testing,\n",
    "                   sleep_time_between_steps_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of stored q value results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.style.use('seaborn-deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"results_qlearning/q_table.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_change</th>\n",
       "      <th>speed_up</th>\n",
       "      <th>speed_up_up</th>\n",
       "      <th>slow_down</th>\n",
       "      <th>slow_down_down</th>\n",
       "      <th>position</th>\n",
       "      <th>velocity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.225814</td>\n",
       "      <td>-1.921098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.614486</td>\n",
       "      <td>-3.257978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.597806</td>\n",
       "      <td>-0.661802</td>\n",
       "      <td>-0.672554</td>\n",
       "      <td>-0.746185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-0.395623</td>\n",
       "      <td>-0.388159</td>\n",
       "      <td>-0.543423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.288238</td>\n",
       "      <td>-0.388159</td>\n",
       "      <td>-0.927818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.181121</td>\n",
       "      <td>-0.388159</td>\n",
       "      <td>-0.543423</td>\n",
       "      <td>-0.198000</td>\n",
       "      <td>-0.799103</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>-0.800000</td>\n",
       "      <td>-2.469936</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.388159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    no_change  speed_up  speed_up_up  slow_down  slow_down_down  position  \\\n",
       "0   -1.225814 -1.921098     0.000000  -1.614486       -3.257978       0.0   \n",
       "1   -0.597806 -0.661802    -0.672554  -0.746185        0.000000       1.0   \n",
       "82  -0.395623 -0.388159    -0.543423   0.000000        0.000000       1.0   \n",
       "44  -0.288238 -0.388159    -0.927818   0.000000        0.000000       2.0   \n",
       "16  -0.181121 -0.388159    -0.543423  -0.198000       -0.799103       2.0   \n",
       "..        ...       ...          ...        ...             ...       ...   \n",
       "54  -0.800000 -2.469936     0.000000  -0.388159        0.000000      18.0   \n",
       "37   0.000000  0.000000     0.000000   0.000000        0.000000      19.0   \n",
       "69   0.000000  0.000000     0.000000   0.000000        0.000000      19.0   \n",
       "27   0.000000  0.000000     0.000000   0.000000        0.000000      19.0   \n",
       "11   0.000000  0.000000     0.000000   0.000000        0.000000      19.0   \n",
       "\n",
       "    velocity  \n",
       "0        3.0  \n",
       "1        1.0  \n",
       "82       0.0  \n",
       "44       0.0  \n",
       "16       2.0  \n",
       "..       ...  \n",
       "54       1.0  \n",
       "37       2.0  \n",
       "69       1.0  \n",
       "27       3.0  \n",
       "11       4.0  \n",
       "\n",
       "[88 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
